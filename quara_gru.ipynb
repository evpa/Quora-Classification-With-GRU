{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Embedding, CuDNNGRU\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With num_words we take most popular 500000 words in the dataset. Also every question will be 50 tokens long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 500000\n",
    "max_tokens = 50\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../input/train.csv')\n",
    "dataset_test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dataset['target'].values.tolist()\n",
    "x_train = dataset['question_text'].values.tolist()\n",
    "x_test = dataset_test['question_text'].values.tolist()\n",
    "\n",
    "data = x_train + x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the questions. Every question will consist of 50 tokens. If a question has less than 50 tokens we will add 0 padding for the missing tokens. If question has more than 50 tokens we cut the extra tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedding we're using Stanford's pretrained glove model. If a word isn't in the glove file it will be randomly initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "with open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt', encoding='UTF-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        values = line[:-1].split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[-300:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "\n",
    "embedding_matrix = np.random.uniform(-1, 1, (num_words, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model. We are using 3 layers of GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False,\n",
    "                    name='embedding_layer'))\n",
    "\n",
    "model.add(CuDNNGRU(units=32, return_sequences=True))\n",
    "model.add(CuDNNGRU(units=32, return_sequences=True))\n",
    "model.add(CuDNNGRU(units=32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = RMSprop(lr=1e-3)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model for 10 epochs with batch size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1306122/1306122 [==============================] - 110s 84us/step - loss: 0.1153 - acc: 0.9546\n",
      "Epoch 2/10\n",
      "1306122/1306122 [==============================] - 106s 81us/step - loss: 0.1039 - acc: 0.9589\n",
      "Epoch 3/10\n",
      "1306122/1306122 [==============================] - 105s 81us/step - loss: 0.1000 - acc: 0.9602\n",
      "Epoch 4/10\n",
      "1306122/1306122 [==============================] - 107s 82us/step - loss: 0.0973 - acc: 0.9612\n",
      "Epoch 5/10\n",
      "1306122/1306122 [==============================] - 105s 81us/step - loss: 0.0952 - acc: 0.9618\n",
      "Epoch 6/10\n",
      "1306122/1306122 [==============================] - 106s 81us/step - loss: 0.0934 - acc: 0.9625\n",
      "Epoch 7/10\n",
      "1306122/1306122 [==============================] - 105s 81us/step - loss: 0.0921 - acc: 0.9630\n",
      "Epoch 8/10\n",
      "1306122/1306122 [==============================] - 108s 83us/step - loss: 0.0908 - acc: 0.9634\n",
      "Epoch 9/10\n",
      "1306122/1306122 [==============================] - 106s 81us/step - loss: 0.0896 - acc: 0.9639\n",
      "Epoch 10/10\n",
      "1306122/1306122 [==============================] - 105s 81us/step - loss: 0.0885 - acc: 0.9644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e07d1170b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad, y_train, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we test the model on test set and create a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x=x_test_pad)\n",
    "cls_pred = np.array([1 if p > 0.5 else 0 for p in y_pred])\n",
    "dataset_test['prediction'] = cls_pred.T\n",
    "df_sub = dataset_test.drop('question_text', axis=1)\n",
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
